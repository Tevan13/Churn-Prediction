{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  RandomSearch Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tuning(data_train, data_test, predictors, target_label, params_path): \n",
    "    '''\n",
    "    Definition : This function takes several inputs in order to do randomsearch hyperparameter optimization for classification problem\n",
    "                 by using XGBoost Classifier\n",
    "    \n",
    "    Input :\n",
    "    1.data_train    : data training variable\n",
    "    2.data_test     : data testing variable\n",
    "    3.predictors    : list of variable predictors\n",
    "    4.target_label  : column name of target variable usually in string\n",
    "    5.params_path   : path to saved the best params\n",
    "    \n",
    "    Output :\n",
    "    1.best_scores   : best performance score depends on what metrics used\n",
    "    2.best_params   : best hyperparameters result\n",
    "    '''\n",
    "    \n",
    "    param_tuning = {\n",
    "        'learning_rate': [i/100.0 for i in range(1,21,1)],\n",
    "        'n_estimators': range(100, 210, 10),\n",
    "        'max_depth': range(1, 11, 1),\n",
    "        'gamma': [i/10.0 for i in range(1,10,1)],\n",
    "        'reg_alpha': [i/1000.0 for i in range(0,100,1)],\n",
    "        'reg_lambda': [i/100.0 for i in range(0,1000,10)]\n",
    "    }\n",
    "\n",
    "    fit_params = {\n",
    "        'eval_metric': 'auc',\n",
    "        'early_stopping_rounds': 100,\n",
    "        'eval_set': [(data_test[predictors], data_test[target_label])],\n",
    "    }\n",
    "\n",
    "    ## RANDOMIZED PARAMETER TUNING ##\n",
    "    XGB_tune = xgb.XGBClassifier(objective='binary:logistic', scale_pos_weight=(len(data_train[data_train[target_label] == 0])/len(data_train[data_train[target_label] == 1])))\n",
    "    random_search = RandomizedSearchCV(XGB_tune, param_tuning, n_iter=50, n_jobs=4, cv=10, verbose=2, refit=True, random_state=24)\n",
    "\n",
    "    print(\"Randomized Search...\")\n",
    "    search_time_start = time.time()\n",
    "    random_search.fit(data_train[predictors], data_train[target_label], **fit_params)\n",
    "    print(\"Randomized Search Time: \", time.time() - search_time_start)\n",
    "\n",
    "    best_score = random_search.best_score_\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    ## SAVE BEST PARAMETERS INTO FILE\n",
    "    save_params = open(params_path,\"w\")\n",
    "    save_params.write(str(best_params))\n",
    "    save_params.close()\n",
    "\n",
    "    return best_score, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 GridSearch Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_tuning(data_train, data_test, predictors, target_label, params_path):\n",
    "    '''\n",
    "    Definition : This function takes several inputs in order to do GridSearch hyperparameter optimization for classification problem\n",
    "                 by using XGBoost Classifier\n",
    "    \n",
    "    Input :\n",
    "    1.data_train    : data training variable\n",
    "    2.data_test     : data testing variable\n",
    "    3.predictors    : list of variable predictors\n",
    "    4.target_label  : column name of target variable usually in string\n",
    "    5.params_path   : path to saved the best params\n",
    "    \n",
    "    Output :\n",
    "    1.best_scores   : best performance score depends on what metrics used\n",
    "    2.best_params   : best hyperparameters result\n",
    "    '''\n",
    "    \n",
    "    param_tuning = {\n",
    "        'learning_rate': [i/100.0 for i in range(1,11,1)],\n",
    "        'n_estimators': range(100, 210, 20),\n",
    "        'max_depth': range(1, 11, 2),\n",
    "        'gamma': [i/100.0 for i in range(0,100,1)]\n",
    "    }\n",
    "    \n",
    "    ## GRID PARAMETER TUNING ##\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator= xgb.XGBClassifier(\n",
    "            learning_rate=0.1, \n",
    "            n_estimators=160, \n",
    "            max_depth=5,\n",
    "            objective= 'binary:logistic', \n",
    "            nthread=4, \n",
    "            scale_pos_weight=(len(data_train[data_train[target_label] == 0])/len(data_train[data_train[target_label] == 1])), \n",
    "            seed=25\n",
    "        ),\n",
    "        param_grid = param_tuning, scoring='roc_auc', n_jobs=4, cv=5\n",
    "    )\n",
    "\n",
    "    grid_search.fit(data_train[predictors], data_train[target_label])\n",
    "    best_score = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    ## SAVE BEST PARAMETERS INTO FILE\n",
    "    save_params = open(params_path,\"w\")\n",
    "    save_params.write(str(best_params))\n",
    "    save_params.close()\n",
    "\n",
    "    return best_score, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to special data format\n",
    "# https://xgboost.readthedocs.io/en/latest/python/python_intro.html\n",
    "dtrain = xgb.DMatrix(data_train[predictors], data_train[target_label], feature_names=predictors)\n",
    "\n",
    "def hyp_xgb(max_depth, subsample, colsample_bytree,min_child_weight, gamma, learning_rate,\n",
    "            max_delta_step, reg_alpha, reg_lambda, scale_pos_weight):\n",
    "    params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric':'auc',\n",
    "    'nthread':-1\n",
    "     }\n",
    "    \n",
    "    params['max_depth'] = int(round(max_depth))\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    params['learning_rate'] = learning_rate\n",
    "    params['max_delta_step'] = int(max_delta_step)\n",
    "    params['reg_alpha'] = reg_alpha\n",
    "    params['reg_lambda'] = reg_lambda\n",
    "    params['scale_pos_weight'] = scale_pos_weight\n",
    "    \n",
    "    scores = xgb.cv(params, dtrain, num_boost_round=500,verbose_eval=False, \n",
    "                    early_stopping_rounds=10, nfold=5)\n",
    "    return scores['test-auc-mean'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds ={\n",
    "    'min_child_weight':(3, 20),\n",
    "    'gamma':(0, 10),\n",
    "    'subsample':(0.5, 1),\n",
    "    'colsample_bytree':(0.1, 1),\n",
    "    'max_depth': (2, 15),\n",
    "    'learning_rate': (0.01, 0.5),\n",
    "    'max_delta_step':(1,30),\n",
    "    'reg_alpha':(0.01,0.4),\n",
    "    'reg_lambda':(.01,.4),\n",
    "    'scale_pos_weight':(.1,20)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(hyp_xgb, pds, random_state=1)\n",
    "optimizer.maximize(init_points=4, n_iter=25)\n",
    "\n",
    "optimizer.max['params']\n",
    "\n",
    "best_params_path_bay = \"%s_best_params_tuning_bayesian.txt\" % str(date.today().strftime(\"%Y%m%d\"))\n",
    "\n",
    "## SAVE BEST PARAMETERS INTO FILE\n",
    "save_params = open(best_params_path_bay,\"w\")\n",
    "save_params.write(str(optimizer.max['params']))\n",
    "save_params.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
